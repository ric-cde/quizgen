# Scope

Title: Emendatio -- Test and hone your knowledge in any field.

Pain point: LLMs are excellent learning tools that can be used to generate quizzes. But using them for this purpose is prone to friction and inconsistency. E.g. every time you want to quiz yourself on a topic, you may need to write a detailed prompt with instructions, context (who is the user), source data, etc. And if you want to quiz yourself on the same subject continuously - e.g. repeatedly in a single chat - the context window can become clogged.

Description: Emendatio allows you to feed in your topic or domain and upload source documents. A series of questions or dialogs helps you create a quiz tailored to your needs. Quizzes are generated according to rigorous standards and automated requirements gathering.

# Tech stack

-   React + Vite
-   Node/Express?
-   OpenAI API and/or AI SDK
-   DB TBA - MongoDB, Supabase, or Postgres.

# Main Features

1. Quiz designer: captures what user wants to learn.

2. Quiz generator (question set).

3. Quiz presenter (user interaction, answering questions).

4. Quiz scorer - rating + showing user responses.

5. Storing all previous quiz results.

# Functional Requirements

-   Capture user input
    -   Field/domain
    -   Topic(s)
    -   Background knowledge
    -   Goal (e.g. pass Leaving Certificate).
    -   Difficulty?
    -   Upload files
-   Generate a list of questions (store in JSON)
-   Display questions one by one and prompt an answer
-   Keep track of user answers
-   Generate final screen with score

    -   Prompt to retake quiz? Or generate a new one?
    -   v6: system adapts & becomes harder/easier depending on responses

-   Statistics
-   Display charts showing user performance over time

---

# Structure & UI

## Data

User
username,
email,
id,

Quiz
id,
questionSets: []
questionSetId
questions: []
questionId
question
topic
sampleAnswer
attempts: []
attemptId
answers:
text
feedback
score
domain,
topic,
purpose,
backgroundKnowledge,
difficulty: easy | standard | hard,
files: []
title
url

## Information Architecture

### Story (UI)
Solo student practicing topic X.
Reqs: Quickly generate a quiz and do it. Repeat/iterate quizzes. User can see how/why they got answer wrong. Can adjust difficulty on subsequent quizzes.


### UI walkthrough
[Generate quiz] button (empty box next to it). See list of previous quizzes (cards), % correct. (clicking existing quiz takes you to compose page for that quiz)
Creating brand new quiz captures quiz info. [Generate] adds quiz to memory, and takes you straight into newly generated quiz.
As you do quiz, shows your progress (e.g. 2/5). 

When quiz is finished, adds results to memory, shows pop-up with results. 
    Shows % correct, time taken.
    Options: [Retake] (same questions), [Generate more questions], [Random], [Home].

Quiz create and quiz update/extend are same page: Compose. (different props/options).
        Topic, description (greyed out; possible edit icon toggle + warning: "updating won't affect previously generated questions"
        GENERATE (on/off): # questions, difficulty.
        DO: Toggles or selectors (button) for 'Existing' / 'New questions' (if generate is toggled, make default) / 'New & existing mix'. (STRETCH: 'New & hard' e.g. tries <= 1, success <= 60%' ).

STRETCH
    - topic has dedicated page where you can see: a) stats, b) list of the questions (inspect answers, etc.)
    - Results shows all questions & possible answers (questionSet explorer). Allows user to edit. Use mindWipe (MiB)
    - sort home page quiz 
    - toggle question types. (short answer, MCQ, true/false)
    - Quiz runner has a "get hint" button for each prompt. (Hint is generated when questions originally created).

### Components

Home (/home)
    TopicQuickStart
        - input: topic + [Generate quiz] button
    QuizList
        ExistingQuiz
            - empty = "You have no quizzes yet. [Generate a quiz]"
            - card with topic, number of questions, description (snippet), (% correct)
            - should be obviously clickable
        

Compose (/compose or /quiz/:id/compose)
    QuizComposer
        - data: quizSession
        - prop: mode (new, expand, rerun)
        - input: topic
            - edit icon; locked if new (or /compose route)
        - input: description
            - edit icon; locked if new (or /compose route)

        GenerateDetails
            - toggle: on/off
                - (forced on if new quiz, can't be disabled)
                - slider: # of questions
                - selector: difficulty
                - grade?: difficulty
                - quizType: MCQ / short answer / trueFalse

        QuizRunDetails
            - (hidden or disabled for new quiz)
            - tab/button selector: 'Existing' / 'New questions' / 'New & existing mix'
                - description snippet appears below depending on selection
            - # of questions

        - [Generate quiz]

QuizRunner
    - data: quizSession (tracks all state)
    QuestionBox
        - animated on render, word by word

    AnswerBox
        - input: answer
        - [Submit] button icon

    QuizResults
        - (modal)
        - results text
        QuestionResult (list)
            - question
            - answer (coloured + marked "Correct! ✅" or "Incorrect! ❌")
            - correct answers
        - [Retake] (same question tranche)
        - [Generate more questions]
        - [Random]
        - [Home]

    if 'Home' clicked or user navigates away...
        ExitQuiz
            "Are you sure you want to exit?"
        
Layout
    Navbar
        [Home]
        [Settings]

Settings
    seeAnswersAtValidation (show correct answers immediately after answer submitted, or only at end)


### Data model (updates)

quizSessions/rounds:
- create a quizSessionId (persist it in localStorage) (??)
    version 1:
        - simply store current quizSession in sessionStorage (fallback, debounced?), run quiz at /quiz/:sessionId/play
        - Has a status (ready, inProgress, complete).
        - QuizRunner checks state, or falls back to sessionStorage.
    verison 2:
        - mirror quizSession to localStorage. Stored in array/object.
        - Navigating to sessionId URL either shows start (ready), active question (inProgress), or a finish screen.
        - All existing quizSessions stored in a table/array/object.
    version 3:
        - when QuizRunner mounts, fetches quizId data via API

handleSubmit / selectQuestions
    version 1:
    - quizSessionId generated
    version 2:
    - POST form state ({questionMix: "new", newQuestionCount: 5}) to api/quiz-generator
    - quiz-generator is mocked to service running locally. (replaced for future version 3 with full API)
    - this places data in quizSession and stores it in localStorage (as above)


<!-- ~~## Pages

» Home
    list: Existing Quizzes cards (if any) 
        btn(s) DO QUIZ | VIEW (stats, etc.)
    btn CREATE QUIZ

» Quiz Page | (VIEW a single quiz) 
    btns(s) DO QUIZ (existing qSet)
    statistics (# of questions, success %)

» Quiz Create/Edit Page
    if new...
        form:
            topic
            description (optional)
            # of questions
            difficulty
            btn GENERATE QUIZ -> START QUIZ
    if existing...
        stats (# of questions, success %)
        btn START QUIZ
        form:
            topic (locked or filled in/editable with pencil icon)
            description (locked or editable)
            # of additional questions
            difficulty of new questions
            btn GENERATE QUESTIONS -> START QUIZ~~ -->


## Wireframe

https://wireframe.cc/yGMi77


---

# Roadmap

### Planning

-   Write list of features, functional reqs.
-   Choose tech stack.

### v0

-   Runs locally in Node. (No LLM API call).
-   Q&A in CLI for selecting/configuring quiz. Generate Quiz function.
-   Pre-existing data (JSON) which populates quiz source data.
-   MVP = 5 questions
-   Can determine # of questions
-   Quiz scoring based on correct answers
-   Single session score with statistics

### v1

-   Quiz questions generated by LLM API. (OpenAI?)
-   Testing: unit tests, integration tests. 

### v2

-   Step by step plan for UI, app (based on v0/v1)
-   React + Tailwind for UI.
-   UI populates with dummy data (new qSet, existing qSets) + makes calls

### v3

-   Refactor backend as simple llm service/API
-   Backend connects to UI
-   CI/CD pipelines (AWS)
-   Data still stored in-memory only

### v4

-   QoL improvements (eval/validation of answers).
-   Runs in cloud now (i.e. llm service) but client stil local
-   Rate limiting for API (cap it at x amount)

### v5

-   Add user log in?
-   Quiz sets persisted in database

### v6

-   User requirements capture stewarded by LLM API.
-   Advanced features (quiz generation, open-endedness, MCQ mode) 

## Future feature musings (ignore for now)

-   Modes: Q&A with canned list of acceptable answers, multiple choice, dynamic (LLM evals answer)
-   After reqs, quiz shows 1-3 questions and does initial survey (fine-tuning). Asks user if questions appropriate. If not, second (more detailed) reqs gathering process, or if it did OK: minor tweaks.
-   When user inputs information, an LLM call rewrites prompt. (Goal: "im doin my leaving cert" => "User is studying for their state-level secondary school exams in Ireland, called the Leaving Certificate). And e.g. further information could be pulled in, e.g. a chained prompt where the AI retrieves a paragraph describing the Leaving Certificate.
-   Combo UI. Reqs gathering will have selectors/check boxes, but also a prompt box where users can write whatever they want at any point.

-   `item response theory or at least logic like “if user misses Q3, ask easier Q3-variant next.”`
-   After each quiz, new eval: either based on user score, or rating by user. (10/10 = too easy, show the user something more difficult).
-   Post quiz survey. Was the quiz useful to your goal? Questions challenging / too easy / nonsensical? Any answers that were wrong?

-   Spaced repetition.

-   Way of fetching/generating images for questions.

#### Answer validity/accuracy

-   Basic (deterministic) check ran against answers for spelling. E.g. if characters 95% correct, then Google-style correction: you enter "van djik" and a hyperlink appears for "did you mean van dijk?")
-   Dispute button between each answer validation: if user thinks their answer was correct, they click dispute. LLM call checks to see if answer could also work. I.e. "van dijk", "virgil van dijk". User writes "vvd". Call 4.1-nano `Could ${userAnswer} be accepted as a correct answer in a quiz for ${question}. Other correct answers: ${}. Topic is ${topic}.
-   Eval for how accurate a quiz + scoring is likely to be. I.e. does the LLM actually know a lot about this subject? e.g. if user enters something obscure.
-   ` deterministic evaluation mode (temperature = 0) for scoring and a creative mode for generation.`

-   Evaluating answers: if user answer is wrong, new LLM call (with topic, question, valid answers) to determine if user answer correct.
-   De-duplication: when generating new question tranches, also submit the array of old questions so model can avoid repeating itself.
-   LLM call first generates "chain-of-thought" text about what kinds of questions would be good to challenge user on topic X. Then next call (or field) is the JSON response.
-       User enters difficulty level (e.g. easy, phd-level, etc.). LLM interpolates and tracks it on a 10-point scale, attached to each question object. (Allows futuer difficulty tracking, moving on to a harder scale, etc.).

## Optional stack elements

# Tutorials

https://codelabs.developers.google.com/cloud-genai
https://arxiv.org/abs/2503.14662?utm_source=chatgpt.com
https://www.youtube.com/watch?v=riDzcEQbX6k


---

# Task log

### v0

// choose 5 questions at random and store in array [✔️]

// loop through the array, posing each question. [✔️]

// Write 1 or 0 to score. [✔️]

// calculate score [✔️]

// present score. [✔️]

// ask if user wants to continue w/ another question set [✔️]

// fix: move topic list to before requestTopic execution [✔️]

// let user type custom for a custom topic, then type in the topic [✔️]

// STRETCH: let user choose how many questions [✔️]

// cleanup: validate user input for # of questions is: a) a number, b) in range. [✔️]
// Add more error handling (setQuestions) [✔️]

// store responses + scoring in separate object/array (not on questions object), e.g. quizSession {quizId, completed (true), correct, total, score%}. When exiting, generate report of how user did across all rounds [✔️]

// split functions into modules/files [✔️]

### v1

// program makes request to openAI API to generate JSON question set (10 questions), then usual flow (user chooses how many), etc. [✔️]

// transform answer array to lower-case [✔️]

// Ask user: how many questions they'd like to generate. Update llm-service to accommodate [✔️]

// When selecting existing quiz, ask user: do existing questionSet(s) from file or generate new? (+ how many & difficulty?) [✔️]
// fix "get ready to dive deep on undefined" [✔️]
// fix flow where topic exists, user says they want new questions, then generateCustomTopic() asks again if they want extra questions. [✔️]
// ask for # of questions at final stage (if existing?). flow may need re-work [✔️]

// add questionSetId to each question at point of generation. (How to calc? Find last element in array and ++ ?) and difficulty approximation [✔️]

// After quiz, ask user: would you like to do topic again? (then existing Q's or generate new - combined with previous)? [✔️]

// Update LLM service. When generating additional, append new questions to answer array with metadata (time, questionSetId). [✔️]

---

// Track per-user "correct" and "attempted" on each question. Possibly need to extract from quizSession and save to disk after each completed quiz. (Don't need to save other quizSession stats; leave for future DB). [✔️]

// Re-evaluate fields. Topic: what user originally entered. slug: short name that's a unique identifier/URL, etc. Description: LLM generated. Must ensure case matches throughout app / isn't manipulated. [✔️]

---

// Implement unit tests for most functions. [✔️]

// Integration tests

## v2

// Bundler? Install. [✔️]

// Write UI development plan [✔️]

// Information architecture: 1) brainstorm all features and group things together + mark priorities (must be obvious)/secondaries (hide in menu). 2) map the flows (user paths), 3)  (sitemap). [✔️]

// Write out list of components, pages/screens [✔️] 

// Decide on responsibility: how much is UI client responsible for, how much happens on server? List data that needs to be push/pulled []

// Quick, basic wireframe. Mentally walkthrough flow. [✔️]

// Routing: add app/layout.tsx (header + navigation) and app/router.tsx. Write shell of app/pages. [✔️]

// Write minimum scaffold of initial components on each page [✔️]

// Create mock data

// Write functions/pipes for bringing data in (mock)

// Link everything up (break this step down further)




## v3


## v4
// QoL improvements, validation

// Structured Outputs conformity (JSON schema object): https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses []

// explore simple validation of spelling errors (i.e. check if characters in answer are 95% correct - and show spelling issue). 

// if previous questionTranche exists, feed those questions in when LLM generates new questionSet. "Here is a list of  questions you have previously generated about this topic. ${questionSet} Please generate a new questionSet which doesn't repeat questions."

// Validate/eval questions & answers. More checks in original call? Send each Q + answers 1-by-1? Second step to check + verify questions. Check additional correct answers? []