# Scope

Title: Emendatio -- Test and hone your knowledge in any field.

Pain point: LLMs can act as a quiz tutor, but there is a lot of friction and inconsistency. E.g. every time you want to quiz yourself on a topic, you need to write a detailed prompt with instructions, context (who is the user), source data, etc. And if you do this repeatedly in a single chat, the context window can become clogged.

Description: Emendatio allows you to feed in your topic or domain and upload source documents. A series of questions or dialogs helps you create a quiz tailored to your needs. Quizzes are generated according to rigorous standards and automated requirements gathering.

# Tech stack

-   React + Vite
-   Node/Express?
-   OpenAI API and/or AI SDK
-   DB TBA - MongoDB, Supabase, or Postgres. 

# Main Features

1. Quiz designer: captures what user wants to learn.

2. Quiz generator (question set).

3. Quiz presenter (user interaction, answering questions).

4. Quiz scorer - rating + showing user responses.

5. Storing all previous quiz results.

# Functional Requirements

-   Capture user input
    -   Field/domain
    -   Topic(s)
    -   Background knowledge
    -   Goal (e.g. pass Leaving Certificate).
    -   Difficulty?
    -   Upload files
-   Generate a list of questions (store in JSON)
-   Display questions one by one and prompt an answer
-   Keep track of user answers
-   Generate final screen with score
    -   Prompt to retake quiz? Or generate a new one?
    -   v6: system adapts & becomes harder/easier depending on responses

-    Statistics
    - Display charts showing user performance over time

---

# Structure

## Data

User
    username,
    email,
    id,

Quiz
    id,
    questionSets: []
        questionSetId
        questions: []
            questionId
            question
            topic
            sampleAnswer
        attempts: []
            attemptId
            answers:
                text
                feedback
            score
    domain,
    topic,
    purpose,
    backgroundKnowledge,
    difficulty: easy | standard | hard,
    files: []
        title
        url

## Components
TBD

## Pages
TBD

## Routes
TBD

---

# Roadmap

### Planning

-   Write list of features, functional reqs.
-   Choose tech stack.

### v0

-   Runs locally in Node. (No LLM API call).
-   Q&A in CLI for selecting/configuring quiz. Generate Quiz button.
-   Pre-existing data (JSON) which populates quiz source data.
-   MVP = 5 questions

### v1

-   Quiz scoring based on correct answers
-   Inquirer.js??

### v2

-   React for UI.
-   UI populated with dummy data.

### v3

-   Can determine # of questions
-   Backend reconfigured to connect to UI.
-   Still runs locally.

### v4

-   Quiz questions generated by LLM API. (OpenAI?)
-   Rate limiting for API (cap it at x amount)

### v5

-   User requirements capture stewarded by LLM API.

### v6

-   Now runs in cloud + browser
-   Add user log in
-   Quiz sets persisted in database

## Experimental feature musings (ignore for now)

-   When user inputs information, an LLM call rewrites prompt. (Goal: "im doin my leaving cert" => "User is studying for their state-level secondary school exams in Ireland, called the Leaving Certificate). And e.g. further information could be pulled in, e.g. a chained prompt where the AI retrieves a paragraph describing the Leaving Certificate.
-   Multiple calls which evaluate and re-evaluate the data, ultimately transforming it into a system prompt for the quiz.
-   Way of fetching/generating images for questions.
-   After reqs, quiz shows 1-3 questions and does initial survey (fine-tuning). Asks user if questions appropriate. If not, second (more detailed) reqs gathering process, or if it did OK: minor tweaks.
-  `item response theory or at least logic like “if user misses Q3, ask easier Q3-variant next.”`
-   After each quiz, new eval: either based on user score, or rating by user. (10/10 = too easy, show the user something more difficult).
-   Post quiz survey. Was the quiz useful to your goal? Questions challenging / too easy / nonsensical? Any answers that were wrong?
-   Combo UI. Reqs gathering will have selectors/check boxes, but also a prompt box where users can write whatever they want at any point.
- Eval for how accurate a quiz + scoring is likely to be. I.e. does the LLM actually know a lot about this subject? e.g. if user enters something obscure.
- ` deterministic evaluation mode (temperature = 0) for scoring and a creative mode for generation.`
- Spaced repetition.

## Optional stack elements



    
# Tutorials

https://codelabs.developers.google.com/cloud-genai
https://arxiv.org/abs/2503.14662?utm_source=chatgpt.com
https://www.youtube.com/watch?v=riDzcEQbX6k
