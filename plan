# Scope

Title: Emendatio -- Test and hone your knowledge in any field.

Pain point: LLMs can act as a quiz tutor, but there is a lot of friction and inconsistency. E.g. every time you want to quiz yourself on a topic, you need to write a detailed prompt with instructions, context (who is the user), source data, etc. And if you do this repeatedly in a single chat, the context window can become clogged.

Description: Emendatio allows you to feed in your topic or domain and upload source documents. A series of questions or dialogs helps you create a quiz tailored to your needs. Quizzes are generated according to rigorous standards and automated requirements gathering.

# Tech stack

-   React + Vite
-   Node/Express?
-   OpenAI API and/or AI SDK
-   DB TBA - MongoDB, Supabase, or Postgres.

# Main Features

1. Quiz designer: captures what user wants to learn.

2. Quiz generator (question set).

3. Quiz presenter (user interaction, answering questions).

4. Quiz scorer - rating + showing user responses.

5. Storing all previous quiz results.

# Functional Requirements

-   Capture user input
    -   Field/domain
    -   Topic(s)
    -   Background knowledge
    -   Goal (e.g. pass Leaving Certificate).
    -   Difficulty?
    -   Upload files
-   Generate a list of questions (store in JSON)
-   Display questions one by one and prompt an answer
-   Keep track of user answers
-   Generate final screen with score

    -   Prompt to retake quiz? Or generate a new one?
    -   v6: system adapts & becomes harder/easier depending on responses

-   Statistics
-   Display charts showing user performance over time

---

# Structure

## Data

User
username,
email,
id,

Quiz
id,
questionSets: []
questionSetId
questions: []
questionId
question
topic
sampleAnswer
attempts: []
attemptId
answers:
text
feedback
score
domain,
topic,
purpose,
backgroundKnowledge,
difficulty: easy | standard | hard,
files: []
title
url

## Components

TBD

## Pages

TBD

## Routes

TBD

---

# Roadmap

### Planning

-   Write list of features, functional reqs.
-   Choose tech stack.

### v0

-   Runs locally in Node. (No LLM API call).
-   Q&A in CLI for selecting/configuring quiz. Generate Quiz function.
-   Pre-existing data (JSON) which populates quiz source data.
-   MVP = 5 questions
-   Can determine # of questions
-   Quiz scoring based on correct answers
-   Single session score with statistics

### v1

-   Quiz questions generated by LLM API. (OpenAI?)
-   ~~- Inquirer.js??~~

### v2

-   React for UI.
-   UI populated with dummy data.

### v3

-   Backend reconfigured to connect to UI.
-   Data stored in-memory only
-   Runs locally.

### v4

-   Runs in cloud now (i.e. llm service)
-   Rate limiting for API (cap it at x amount)

### v5

-   Add user log in
-   Quiz sets persisted in database

### v6

-   User requirements capture stewarded by LLM API.

## Experimental feature musings (ignore for now)

-   Dispute button between each answer validation: if user thinks their answer was correct, they click dispute. LLM call checks to see if answer could also work.
    i.e. "van dijk", "virgil van dijk". User writes "vvd". Call 4.1-nano `Could ${userAnswer} be accepted as a correct answer in a quiz for ${question}. Other correct answers: ${}. Topic is ${topic}.
-   When user inputs information, an LLM call rewrites prompt. (Goal: "im doin my leaving cert" => "User is studying for their state-level secondary school exams in Ireland, called the Leaving Certificate). And e.g. further information could be pulled in, e.g. a chained prompt where the AI retrieves a paragraph describing the Leaving Certificate.
-   Multiple calls which evaluate and re-evaluate the data, ultimately transforming it into a system prompt for the quiz.
-   Way of fetching/generating images for questions.
-   After reqs, quiz shows 1-3 questions and does initial survey (fine-tuning). Asks user if questions appropriate. If not, second (more detailed) reqs gathering process, or if it did OK: minor tweaks.
-   Modes: Q&A with canned list of acceptable answers, multiple choice, dynamic (LLM evals answer)
-   `item response theory or at least logic like “if user misses Q3, ask easier Q3-variant next.”`
-   After each quiz, new eval: either based on user score, or rating by user. (10/10 = too easy, show the user something more difficult).
-   Post quiz survey. Was the quiz useful to your goal? Questions challenging / too easy / nonsensical? Any answers that were wrong?

-   Combo UI. Reqs gathering will have selectors/check boxes, but also a prompt box where users can write whatever they want at any point.
-   Eval for how accurate a quiz + scoring is likely to be. I.e. does the LLM actually know a lot about this subject? e.g. if user enters something obscure.
-   ` deterministic evaluation mode (temperature = 0) for scoring and a creative mode for generation.`
-   Spaced repetition.
-   Spell check ran against answers. (Google-style - e.g. you enter "van djik" and a hyperlink appears for "did you mean van dijk?")
-    Evaluating answers: if user answer is wrong, new LLM call (with topic, question, valid answers) to determine if user answer correct.
-   LLM call first generates "chain-of-thought" text about what kinds of questions would be good to challenge user on topic X. Then next call (or field) is the JSON response.
-     User enters difficulty level (e.g. easy, phd-level, etc.). LLM interpolates and tracks it on a 10-point scale, attached to each question object. (Allows futuer difficulty tracking, moving on to a harder scale, etc.).

## Optional stack elements

# Tutorials

https://codelabs.developers.google.com/cloud-genai
https://arxiv.org/abs/2503.14662?utm_source=chatgpt.com
https://www.youtube.com/watch?v=riDzcEQbX6k

# Task log

### v0

// choose 5 questions at random and store in array [✔️]

// loop through the array, posing each question. [✔️]

// Write 1 or 0 to score. [✔️]

// calculate score [✔️]

// present score. [✔️]

// ask if user wants to continue w/ another question set [✔️]

// fix: move topic list to before requestTopic execution [✔️]

// let user type custom for a custom topic, then type in the topic [✔️]

// STRETCH: let user choose how many questions [✔️]

// cleanup: validate user input for # of questions is: a) a number, b) in range. [✔️]
// Add more error handling (setQuestions) [✔️]

// store responses + scoring in separate object/array (not on questions object), e.g. quizSession {quizId, completed (true), correct, total, score%}. When exiting, generate report of how user did across all rounds [✔️]

// split functions into modules/files [✔️]

### v1

// program makes request to openAI API to generate JSON question set (10 questions), then usual flow (user chooses how many), etc. [✔️]

// transform answer array to lower-case [✔️]

// Ask user: how many questions they'd like to generate. Update llm-service to accommodate [✔️]

// When selecting existing quiz, ask user: do existing questionSet(s) from file or generate new? (+ how many & difficulty?) [✔️]
    // fix "get ready to dive deep on undefined" [✔️]
    // fix flow where topic exists, user says they want new questions, then generateCustomTopic() asks again if they want extra questions. [✔️]
    // ask for # of questions at final stage (if existing?). flow may need re-work [✔️]


// add questionSetId to each question at point of generation. (How to calc? Find last element in array and ++ ?) and difficulty approximation [✔️]

// After quiz, ask user: would you like to do topic again? (then existing Q's or generate new - combined with previous)? [✔️]

// Update LLM service. When generating additional, append new questions to answer array with metadata (time, questionSetId). [✔️]

---

// Track per-user "correct" and "attempted" on each question. Possibly need to extract from quizSession and save to disk after each completed quiz. (Don't need to save other quizSession stats; leave for future DB)

// Re-evaluate fields. Topic: what user originally entered. ShortName: (possibly truncated by LLM?). Description: LLM generated. Must ensure case matches throughout app / isn't manipulated.

// Implement unit tests for most functions

// Structured Outputs conformity (JSON schema object): https://platform.openai.com/docs/guides/structured-outputs?api-mode=responses []

// explore simple validation of spelling errors (i.e. check if characters in answer are 95% correct - and show spelling issue).

// Validate/eval questions & answers. More checks in original call? Send each Q + answers 1-by-1? Second step to check + verify questions. Check additional correct answers? []

// v2

// Next vs. Vite? Install.

// Map out layout + potential components.
